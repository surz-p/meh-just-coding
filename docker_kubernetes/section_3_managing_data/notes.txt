Content of this chapter:
- Understanding different kinds of data
- Images, Containers and Volumes
- Using arguments and environment variables

What can Data be?
- Application (Code + Environment; Image layer) [last 2 chapters] - READ ONLY
- Temporary App Data - READ / WRITE (Container layer)
    - This is data that is fetched/produced in the running container
    - This is data that is not needed after the container is stopped and
      hence discarded
    - e.g., form user input that might be stored in a temporary file
- Permanent App Data - READ / WRITE (Container + Volume layer)
    - This is data that is needed even after the container is stopped
    - Even more interesting: data that is needed even after container deletion!
    - e.g., database data, log files, user accounts etc.

Let's take the following example
- We have a web application that stores user feedback in a file
- The filename is title.txt where "title" is entered by the user in form input
- When the user enters a title, the file is created in a "temp" folder
- If the file created is not already present in a "feedback" folder,
  it is moved there
- If the file created is already present in the "feedback" folder, we just
  leave the file in the "temp" folder and don't move it to the "feedback" folder
- At the end of the day, we want everything from the "feedback" folder and will
  discard everything from the "temp" folder
- This example is a simple way to understand the different kinds of data and
  contains all the above mentioned data types i.e., Application, Temporary and
  Permanent data

Stop/Start Container
- When the container is stopped and restarted, we still have the files in the
  "temp" folder and the files in the "feedback" folder
- This is because the container has a read/write layer of the file system unlike
  the image layer which is read only
- All writing to the file system is done in the container layer

Delete Container
- When the container is deleted, the files in the "temp" and "feedback" folder
  are removed
- This is because the container layer is removed when the container is deleted
- The files are not present in the image layer; thus no effect on the image

Volumes
- Volumes help us with persistent data
- Volumes are holders on your "HOST MACHINE" which are mounted into conainers
- It's like a mapping of a folder inside the container to a folder on the host
- Changes in either folder will be reflected in the other one
- Volumes persist if a container shuts down or is removed.
- If a container starts and mounts a volume, any data inside that volume is
  available in the container
- Containers can read/write data from/to a volume; this is very powerful
- Volumes are added to the Dockerfile by the instruction
  !!! IMPORTANT NOTE !!!
  VOLUME [ "/path/to/dir/inside/the/container" ]
- It is important to note that the path to the volume is relative to the root
  of the file system of the container (inside the container)

Two Types of External Data Storages
- Volumes (managed by Docker)
- Bind Mounts (managed by you)

Volumes, again!
- VOLUME [ "/path/to/dir/inside/the/container" ] creates an anonymous volume
- We can also create named volumes
- Docker sets up a folder/path on your host machine (exact location is unknown
  to us)
- This can be managed via "docker volume" command
- By using anonymous volumes, we can't control the location of the volume on
  the host machine. We don't know where the volume is stored on the host
- When you stop a container (that was started with --rm), the volume is gone!
  This is because the anonymous volume is managed automatically by docker
- However, what I noticed is that the volume is not gone if the container is
  stopped and removed manually using "docker rm <container_id>", if it did
  not originally start with --rm option
- This unremoved volume is called a dangling volume and can be seen using the
  "docker volume ls -f dangling=true" command or just "docker volume ls"
- With named volumes, volumes will survive container shutdowns/removals unlike
  anonymous volumes which are removed when the container is removed
- Anonymous volumes are deleted because they are recreated whenever a container
  is created. Therefore keeping them around after container is gone makes no
  sense
- So, they are ideally good for persistent data
- !!! IMPORTANT NOTE !!!
  We can NOT create a named volume inside of a Dockerfile
- Instead named volume is created when we run a container! An example would be
  "docker run -v <my-named-volume>:</path/to/dir/inside/the/container>"
- Therefore volumes are good for persistent data, that need not be edited

Bind Mounts
- Bind mounts are similar to volumes in some cases but there is one key difference
  i.e., Bind mounts are managed by the user (you) and not by Docker
- While docker manages the named volumes in locations unknown to us, bind mounts
  are managed by us and we can control the location of the bind mount on the host
  machine
- This eliminates a very important problem during development - the problem of
  having to rebuild the image every time we make a change to the code. During dev,
  we often make changes to the src code and we want to see the changes immediately
  But we need to rebuild the image and start a new container to see the changes
- With the help of bind mounts, we can place the src code on at the known path
  and docker can pick up the changes from there (from the bind mount)
- Therefore, bind mounts are perfect for persistent and editable data
- !!! IMPORTANT NOTE !!!
  Bind mount is specific to the container which you run, so we can not add it in
  the Dockerfile
- !!! IMPORTANT NOTE !!!
  To add a bind mount, we specify the "-v" flag just similar to a volume, but
  this time, the ***ABSOLUTE*** path of the mapping folder is specified
  "docker run -v </absolute/path/on/host>:</path/to/dir/inside/the/container>"
- Make sure that Docker has access to the folder in the host file system so that
  it can access the absolute path on the host machine for the bind mount

Understanding Container/Volume interaction
- When a container is started with a volume or a bind mount, the container data
  is stored in the volume
- At the same time, the volume data is accessible in the container

The dilemma
- Let's say that we run the following command:
  ```docker run -d -p 8080:80 --rm --name ch3bmount -v <vol-name>:/supernode/<folder>
  -v "/Users/sp03/yada-yada-yada:/supernode" chapter3:nvolumes```
  This command will essentially overwrite the contents of the /supernode folder
  with the contents of the yada-yada-yada directory.
- !!! IMPORTANT NOTE !!!
  This is bad
- Because, the yada-yada-yada folder does not contain the node_modules
  folder and we are essentially overwriting the /supernode dir (which actually
  contains the node_modules folder generated by the Dockerfile instruction),
  with the contents of the yada-yada-yada folder, we are losing essential dependencies

So how can we solve this?
- Anonymous volumes come into picture here
- They can be added with "docker run -v /supernode/node_modules"; instead of adding this
  in the Dockerfile
- Note that it has no colon (:) and no name before the colon
- This should ideally remind you of the "equivalent" VOLUME instruction in the Dockerfile
  like: VOLUME [ "/supernode/node_modules" ]
- !!! IMPORTANT NOTE !!!
  Docker evaluates all volumes you set on the containers and if there are clashes, the
  longer internal path wins
  For e.g.,
  ```docker run -d -p 8080:80 --rm --name ch3bmount -v myVol:/supernode/feedback
  -v "/Users/sp03/my-repo:/supernode" -v /supernode/node_modules chapter3:nvolumes```
  where:
    -v myVol:/supernode/feedback is the named volume
    -v "/Users/sp03/my-repo:/supernode" is the bind mount
    -v /supernode/node_modules is the anonymous volume
  In this case, the /supernode/node_modules will win, meaning it wont be overwritten and
  instead it will overwrite what is coming from the bind mount; thus keeping the
  node_modules folder intact

Read Only Volumes
- A volume can be marked as read only by adding the ":ro" suffix to the volume
  or bind mount
- For e.g.,
  ```docker run -d -p 8080:80 --rm --name ch3bmount -v myVol:/supernode/feedback
  -v "/Users/sp03/my-repo:/supernode:ro" -v /supernode/node_modules chapter3:nvolumes```
- The ":ro" suffix tells docker that it is no longer able to write into that folder
  or its sub folders. However, we do want some data to be written to /supernode/feedback
  and /supernode/temp folders. So, we can add an anonymous volume for these folders
  and leave them as read/write and docker will check the longer internal path and
  give them higher priority so that they can still be used to write data while the
  rest of the folders are read only
- For e.g.,
  ```docker run -d -p 8080:80 --rm --name ch3bmount -v myVol:/supernode/feedback
  -v "/Users/sp03/my-repo:/supernode:ro" -v /supernode/node_modules
  -v /supernode/temp chapter3:nvolumes```
- This will make the bind mount read only and the container will not be able to write
  to the bind mount, while we will still be able to make changes on the host machine

Points to ponder
If we are using a bind mount on the source code, why do we need a COPY instruction
in the Dockerfile if it is anyways going to be overwritten by the bind mount?
- Yes, we can remove the COPY instruction from the Dockerfile. However, we need to
  remember that bind mount was added by us to be able to make changes to the source
  during development, so that we omit the process of rebuilding the image every time
  there is a minor change to the src code. But, when we are ready to deploy the app,
  we will not require constant changes to the src code and we will want to use the
  COPY instruction in the Dockerfile to copy the src code into the image.
- I personally also think that using a bind mount will only let you work locally.
  If the project/image needs to be shared with someone else, they will need an image
  with the src code in it. So, the COPY instruction will come in handy at such times

dockerignore
- The .dockerignore file is similar to the .gitignore file and is used to ignore
  files and folders that we do not want to be copied into the image by the COPY
  instruction

ARGuments and ENVironment Variables
- Environment Variables
- Docker supports build time arguments and run time environment variables
- Arguments are set via the --build-arg option and they are available inside of
  the Dockerfile and NOT accessible via CMD line or any application code
- Environment variables are available in the Dockerfile and the application code
  and can be set via the ENV instruction in the Dockerfile or via the --env flag
  on docker run; i.e., docker run --env <key>=<value>
- For e.g.,
  ```docker run -d -p 8080:8765 --env PORT=8765 --rm --name ch3env chapter3:env```
  This will set the PORT environment variable to 8765, even if you mention the
  ENV instruction in the Dockerfile like ENV PORT=80; The 8765 in command line
  will override the 80 in the Dockerfile
- If there is no override via the command line, then the value in the Dockerfile
  will be used as the default PORT value
- You can also use "-e" instead of "--env" in the command line and have any number
  of environment variables set in the command line like
  "docker run -e PORT=8765 -e OPT=debug ... -e KEY=VAL <image>"
- You can also use the ".env" file to set environment variables and use the --env-file
  flag to set the environment variables from the .env file
- !!! IMPORTANT NOTE !!! (Environment Variables and Security)
  Depending on which kind of data you're storing in your environment variables, you
  might not want to include the secure data directly in your Dockerfile
  Instead, go for a separate environment variables file which is then only used at
  runtime (i.e. when you run your container with docker run)
  Otherwise, the values are "baked into the image" and everyone can read these values
  via "docker history <image>"

- Arguments
- Arguments are defined in the Dockerfile using the ARG instruction for flexibility
  and are set at build time using the --build-arg flag
- An ARG key can not be used in the CMD instruction or in the application code
  because that is only available at build time
- For e.g.,
  ...
  ARG DEFAULT_PORT=80
  ...
  ENV PORT=$DEFAULT_PORT
  ...
  and then build it for multiple ports like
  on PROD = docker build . -t myapp:prod # build with the default port 80
  on DEV = docker build . -t myapp:dev --build-arg DEFAULT_PORT=8765 # with port 8765
  Now we have two different images, built differently for different environments!

- !!! TIP !!!
  Place the ARG and ENV instructions at the bottom of the Dockerfile so that a new
  build with a different variable value does not invalidate the cache for the rest of
  the instructions in the Dockerfile, that are followed after these instructions
